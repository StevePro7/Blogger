Deep Learning
20-Aug-2025

Book by Ian Goodfellow	2016
ISBN
9780262035613

https://www.deeplearningbook.org

//Chp02
//Linear Algebra
//https://www.deeplearningbook.org/contents/linear_algebra.html
 
Chp05
Machine Learning Basics
https://www.deeplearningbook.org/contents/ml.html

Chp09
Convolutional Networks
https://www.deeplearningbook.org/contents/convnets.html

Chp14
Autoencoders
https://www.deeplearningbook.org/contents/autoencoders.html



Chp01
2	AI deep learning
3	ML
4	representation learning
	autoencoder	ENC fn input into rep + DEC fn new rep to input
5	Deep learning
	MLP	Multi Layer Perceptron
6	logistic sigmoid
7	deep probabilistic model
	DL = study of models - greater learned concepts that ML
	DL = represent world as nested hierarchy of concepts
12	DL history cybernetics, connectionism, deep learning
13	ANN = Artificial Neural Network
14	ADALINE	adaptive linear element
15	RLU		rectified linear unit
16	DL != simulate the brain
	distributed representation
17	back-propagation	dominant training deep models
	LSTM	Long Short-Term Memory
18	DL supervised learning algorithm + deep models leverage label DS
19	Big Data
25	RNN Recurrent Neural Networks e.g. LSTM
	Reinforcement Learning	[DL-extension]
	S/W Torch + TensorFlow
26	DL approach to ML w/ applied Math
	
	
Chp02
29	scalars	single number
s ∈ R	s = real-value scalar		slope of line
n ∈ N	n = natural number scalar	no. units

30	vectors	array of numbers
x = [x1, x2, ... xn]
S={1,3,6}		Set of x contains x1, x3, x6
x−S			vector = all x except x1, x3, x6

30	matrices	2D array of numbers
A ∈ Rm×n		matrix A width=m height=n	real-value
Ai,:			i-th row of A
A:,i			i-th col of A

31	tensors	array of numbers > 2x axes
“A”
Ai,j,k			element of A at co-ords (i,j,k)

31	transpose	mirror image across main diagonal line
AT
(AT)i,j= Aj,i

32
x = [x1, x2, x3]T
C = A + B 		add 2x matrices i.e	Ci,j= Ai,j+ Bi,j
D = a . B + c		multiple matrix by scalar

DL
C=A+b			matrix = matrix + vector Ci,j=Ai,j+bj
broadcasting		implicit copying of b to many locations


32	C = AB		matrix multiplication
dot product		btwn two vectors x and y	xTy

33	distributed	A(B + C) = AB + AC
	associative	A(BC) = (AB)C
NOT	commutative	AB != BA
but 	commutative	xTy = yTx
	transpose	(AB)T= BT . AT
i.e.			xTy = (xTy)T = yTx
	
34	identity matrix	In∈ Rn×n
	inverse  matrix	A−1
A−1A = In			matrix inverse multiplied on left

35	linear combination
span	of set of vectors = set of all points obtainable by linear combination

36	linear dependence
	singular	square matrix with linearly dependent columns

37
AA−1= I			matrix inverse multiplied on right

37	Norm		ML = measure size of vectors
norm of vector x = distance from origin to point x

Euclidean norm		L2 norm i.e. p=2
euclidean distance from origin to point identified by x

L2 norm in ML frequent = ||x||	subscript 2 omitted

38
L1 norm in ML when difference btwn zero and nonzero = IMPORTANT
max norm L[infinity]	simplifies abs value element w/ largest magnitude

xTy = ||x||2||y||2cos θ

39	symmetric matrix = matrix = to its own transpose	A = AT
	unit vector = vector with unit norm	length = 1
	orthongonal vector x and y = 90deg	xTy = 0
orthonormal		vectors orthongonal plus have unit form
orthongonal matrix	matrix A inverse = matrix A transpose

40	Eigendecomposition
decompose matrix into a set of eigenvectors and eigenvalues

45	Determinant	= product of all the eigenvalues of the matrix
PCA	Principal Components Analysis


Chp03
51	Probability theory
52	stochastic	nondeterministic
53	frequentist probability	
	Bayesian probability
54	Random variable		
	Probability Distributions
	PMF	Probability Mass Function
56	PDF	Probability Density Function
	;	parameterized by
	marginal probability distribution
57	Conditional probability
	chain rule [product rule] of probability
58	variance
	standard deviation = SQRT( variance )
	co-variance	how much 2x values are linearly related to each other
60	Bernoulli
	Multinoulli	categorical distribution
61	Gaussian	normal distribution
62	central limit theorem
	multivariate normal distribution
63	Exponential distribution
	Laplace distribution
65	latent variable		random variable cannot observe directly
65	logistic sigmoid	saturates = flat when very +ve or -ve
	softplus function
68	logit
	Bayes rule
69	measure theory
71	Information theory


Chp04	Numerical Computation
78	underflow	number close to zero => zero
79	overflow	number large magnitude => inf or -inf
	softmax		associated with multinoulli distribution
80	Theano		S/W package
	Objective function OR criterion		cost fn, loss fn, error fn
superscript x* = arg min f(x)
81	calculus derivative f'(x) = dy/dx
	gradient descent = reduce f(s) by moving x in small steps opposite derivative
82	global minimum
83	directional derivative
	method of steepest descent = gradient descent
84	gradient descent = repeatedly small moves = hill climbing
	Jacobian matrix
87	second derivative test
90	Newton's method	based on second-order Taylor series expansion
91	Lagrangian
Remember Moore-Penrose pseudoinverse is the superscript "+"


Chp05
96	ML = applied statistics
	frequentist estimators + Bayesian inference
DL algorithms based on optimization aglo = stochastic gradient descent
97	Learning algorithms	ML = learn from data
E	Experiences		supervised OR unsupervised
T	Tasks			process of learning [not the task]
P	Performance measures	ML quantitative measure

Tasks
Classification
Classification w/ missing inputs
Regression
Transcription
Machine translation
Structured output
Anomaly detection
Synthesis and sampling
Imputation of missing values
Denoising
Density estimation	probability mass function estimation

101
P	Performance measures	ML quantitative measure
e.g.	accuracy	

102
E	Experiences		supervised OR unsupervised

103
Supervised learning		dataset w/ features
				each example = label OR target
104	Supervised learning	e.g.
regression
classification
RL	algorithms interact w/ an environment
104	Design matrix
each row	different example	vector
each col	different features

105	Linear Regression
106	measure performance = compute MSE of model on test set
	Mean Square Error
108	b = bias = intercept term
108	GENERALIZATION	ability to perform well on unobserved inputs
108	Capacity, Overfitting, Underfitting	
109	i.i.d	independent identically distributed
110	capacity = ability to fit a wide variety of functions
112	Occam's razor	choose simplest hypothesis
114	Bayes error = error incurred making predictions from true distribution