
24-Dec-2024

https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-ML.NET


Chp.01
ASIC	Application-Specific Integrated Circuits
TPU	Tensor Processing Units

2018
Machine learning to data is what refining plants are to oil.

Model Building process
- Problem statement
- Feature engineering
- Obtain dataset
- Feature extraction
- Model training
- Model evaluation


Problem statement
The five Ws of Who, What, When, Where, and Why


Feature engineering
features as
components or attributes of the problem you wish to solve
features are one of the biggest impacts on your
model's performance

feature importance
is used to determine what features are actually driving your predictions


Obtain datasets
A dataset is used to train the model on what the output
should be

overfitting
your dataset is composed of all the same data
points and your model will not be able to properly predict

A diverse but representative dataset is required for machine
learning algorithms to properly build a production-ready model


Feature extraction
Feature extraction, depending on the size of your dataset and your features,
could be one of the most time-consuming elements of the model building process


Model training
After feature extraction, you are now prepared to train your model


Model evaluation
model evaluation is to hold out a portion of your dataset for evaluation. The idea behind this
is to take known data, submit it to your trained model, and measure the efficacy of your model


Supervised ML
passing the known outputs as part of the training to the model.
labelling

Unsupervised ML
typical use case is when figuring out the input
and output labels proves to be difficult


ML Algorithms
- Binary classification
- Regression
- Anomaly detection
- Clustering
- Matrix factorization

Binary classification
output of a model trained with a binary classification algorithm will return a true or false conviction

Regression
return a real value as opposed to a binary algorithm or ones that return from a set of specific values

NB: 
linear		return predicted value
logistic	return probability of outcome occurring

Anomaly detection
looks for unexpected events in the data submitted
to the model

Clustering	Unsupervised ML
offer a unique solution to problems
where finding the closest match to related items is the desired solution.

Matrix factorization
provides a powerful and easy-to-use
algorithm for providing recommendations


ONNX
Open Neural Network eXchange

CNTK
CogNitive ToolKit


Chap.02
create + train ML model

Binary Logistic Regression Classification model
SDCA
Stochastic Dual Coordinate Ascent

SDCA = the algorithm

Project architecture
Model runs


Trainer
model building and evaluation code

Predictor
code to run predictions w/ a trained model


RestaurantFeedback
structured input to feed data pipeline
passed into training phase and trained model

label + text


RestaurantPrediction
output properties from model runs

prediction + probability + score


Trainer
load training data		LoadCSV
builds train and test set	TrainTestSplit
creates pipeline		Fit		train the model
saves model
evaluates model			Evaluate


TTS = 20% or 0.2 by default


Predictor
provides prediction support
run the model given input

load model
chapter02.mdl

create prediction object using input / output
input	RestaurantFeedback,
output	RestaurantPrediction

predict output i.e. -ve or +ve result
based on input


Evaluate model
each model type [algorithm] uses different metrics to concentrate
on when analyzing the performance of a model
e.g.
binary classification

define 4x prediction types
TN	True  negative	  properly classified Neg
TP	True  positive	  properly classified Pos
							CANCER
FN	False negative	improperly classified Neg	game over
FP	False positive	improperly classified Pos  	treatment

IMP!
remember "confusion matrix"


Metric #1	Accuracy
commonly used metric when evaluate model
calculated as ratio of correctly classified
predictions to total classifications

Metric #2	Precision
proportion of true results over all positive results in a model
e.g.
precision = 1	no FP	false positives		ideal
FP e.g. misclassifying file as malignant when is benign

Metric #3	Recall
fraction of all correct results returned by the model
e.g.
recall = 1	no FN	false negatives		ideal
FN e.g. classifying something negative when should be positive

Metric #4	F-score
utilizes both precision and recall
produces weighted average based on FP and FN

give another perspective on model performance
compared to simply looking at accuracy

range 0-1	ideal = 1


AUC
Area Under Curve
area under plotted graph with

X-axis	FP	false positives
Y-axis	TP	 true positives
e.g.
binary classifier model
returned AUC value btwn 0 and 1
i.e.	100%


Average Log Loss
expresses the penalty for wrong results in a single number
taking the difference btwn true classification and the one
the model predicts

Training Log Loss
represents the uncertainty of the model using probability
vs. the known values
NB: low numbers = better


GPT
difference btwn model and algorithm
e.g.
Binary Classification [model]

Binary Classification
type of problem where task = classify data into one of two categories

Binary Classifier
model that is trained to make these two-class predictions

Algorithm
process or method used to train the model
e.g.
logistic regression uses gradient descent to optimize model parameters
support vector machine uses optimization techniques to find best hyperplane


SUMMARY
Binary classifier	model that outputs one of two classes
Algorithm		method used to train the model

e.g.
logistic regression = 	algorithm
when logistic regression [algorithm] used to classify data into two
classes =		binary classifier



Chap.03
Trainers

Regression Model application
Linear Regression	predict employee attrition
Logistic Regression	perform basic static file analysis

regression models
9x linear regression trainers

FastTreeRegressionTrainer
FastTreeTweedieTrainer
FastForestRegressionTrainer
GamRegressionTrainer
LbfgsPoissonRegressionTrainer
LightGbmRegressionTrainer
OlsTrainer
OnlineGradientDescentTrainer
SdcaRegressionTrainer


binary logistic regression trainers:
4x logistic regression trainers

LbfgsLogisticRegressionBinaryTrainer
SdcaLogisticRegressionBinaryTrainer
SdcaNonCalibratedBinaryTrainer
SymbolicSgdLogisticRegressionBinaryTrainer

logistic regression
boolean value
pre-defined range of values

linear regression
unknown numeric value e.g. employment duration


Choose linear regression trainer
FastTree	utilize neighbor joining
LightGBM	utilize GOSS Gradient-based One Side Sampling


Choose logistic regression trainer
all return binary classification

L-BFGS		train + predict in low memory environment
SDCA		optimized for scalability in training
Symbolic	minimizes the error function


Regression Model application #1
Linear Regression

var trainer = MlContext.Regression.Trainers.Sdca();

SDCA
Stochastic	unpredictability to predict error function
Dual Co-ord	two variables coupled when training the model
Ascent		maximizing the value of the error function


Trainer
NormalizeMeanVariance
normalize on both mean and variance
subtract mean from input data and divide value by the variance
nullify outliers so model isn't skewed

Program
train
Project Properties | Debug
train ..\..\..\Data\sampledata.csv

Loss Function: 266.27
Mean Absolute Error: 14.01
Mean Squared Error: 266.27
RSquared: -0.51
Root Mean Squared Error: 16.32

predict
Project Properties | Debug
predict ..\..\..\Data\input.json

Based on input json:
{
"durationInMonths": 0.0,
"isMarried": 0,
"bsDegree": 1,
"msDegree": 1,
"yearsExperience": 2,
"ageAtHire": 29,
"hasKids": 0,
"withinMonthOfVesting": 0,
"deskDecorations": 1,
"longCommute": 1
}
The employee is predicted to work 25.73 months


Regression Model application #2
Logistic regression

var trainer = MlContext.BinaryClassification.Trainers.SdcaLogisticRegression();

true being malicious and false being benign


Trainer
FeaturizeText transform builds NGrams from strings data extracted from files
NGrams
popular method to create vectors from a string and feed into the model
breaks longer string into ranges of characters based on NGram parameter

extract temp_data
train ..\..\..\Data\sampledata.csv

Evaluating regression model
poorly trained model with only provide inaccurate predictions

calcluate model accuracy based on test set at time of training
gives an idea how well model will perform in Prod environment


Regression Metrics
Loss function
Mean absolute error
Mean squared error
R-squared
Root mean squared error


Loss function
set when regression trainer initialized
SDCA default to SquaredLoss NOT TweedieLoss or PoissonLoss


Mean squared error	MSE
measure of average of the square of errors
best used to evaluate models when outliers are critical to prediction


Mean absolute error	MAE
sums the distances btwn points and the prediction lines as
opposed to computing the mean

IMPORTANT
mean bias error
two data points equidistant from the line
one above and one below - this would balance out with
positive and negative value


R-squared		coefficient of determination
represent how accurate the prediction is compared to the test set
calculated by taking the sumo f the distance btwn every data point
and the mean squared - subtracting them and then squaring it

IMP
high values != model performance	overfitting
e.g.
lot of features fed to the model = model more complex
OR
simply not enough diversity in the training and test sets
thus test set = same range of values = overfitting


Root mean squared error	RMSE
distance btwn predicted and actual values
RMSE takes a mean of all those distances, square that value,
and takes the square root
value < 180 = good model

Summary
linear regression
application using SDCA and ML.NET to predict employee attrition

logistic regression
application using SDCA and ML.NET to provide file classification



Chap.04
binary classification
01. car price good or not				0 or 1
02. multi-class classification app categorizes email	range labels

binary classification models

AveragedPerceptronTrainer
SdcaLogisticRegressionBinaryTrainer
SdcaNonCalibratedBinaryTrainer	
SymbolicSgdLogisticRegressionBinaryTrainer
LbfgsLogisticRegressionBinaryTrainer
LightGbmBinaryTrainer
FastTreeBinaryTrainer
FastForestBinaryTrainer
GamBinaryTrainer
FieldAwareFactorizationMachineTrainer
PriorTrainer
LinearSvmTrainer


mulit-class classifiers

LightGbmMulticlassTrainer
SdcaMaximumEntropyMulticlassTrainer
SdcaNonCalibratedMulticlassTrainer
LbfgsMaximumEntropyMulticlassTrainer
NaiveBayesMulticlassTrainer
OneVersusAllTrainer
PairwiseCouplingTrainer

Ex
01.	FastTreeBinaryTrainer
02.	SdcaMaximumEntropyMulticlassTrainer

remember prediction output type will help decide btwn
binary or multi-class classification

best
binary classification trainers
SDCA, LightGBM, and FastTree

multi-class classification trainers
LightGBM and SDCA


Ex01
FastTree based on MART
Multiple Additive Regression Trees
gradient boosting algorithm

Gradient boosting
popular technique in which a series of trees are built in a
step-wise manner before ultimately selecting the best tree

MART
learning an ensemble or regression trees that use scaler
values in their leaves

NB: output
PredictedLabel property contains our classification result


Trainer uses NormalizeMeanVariance

BV:
tweak no. leaves and no. trees to see how both the model
metrics and your prediction probability percentage changes


Program
train ..\..\..\Data\sampledata.csv ..\..\..\Data\testdata.csv

Accuracy: 88.89%
Area Under Curve: 100.00%
Area under Precision recall Curve: 100.00%
F1Score: 87.50%
LogLoss: 2.19
LogLossReduction: -1.19
PositivePrecision: 1
PositiveRecall: .78
NegativePrecision: .82
NegativeRecall: 100.00%

input.json
predict input.json

Based on input json:
{
"HasSunroof":0,
"HasAC":0,
"HasAutomaticTransmission":0,
"Amount":1300
}
The car price is a good deal, with a 100% confidence


Ex02	emails
orders
spam
friend

SdcaMaximumEntropy trainer
SDCA
uses empirical risk minimization which optimizes based
on the training data
potential outliers or anomalies to affect the predict performance
thus
provide trainer w/ ample sampling data to avoid overfitting and
potential errors when predicting the ddata
NB:
SdcaMaximumEntropy trainer
does require normalization

pipeline mapping input properties to FeaturizeText transformations
before appending the SDCA trainer

Program
train ..\..\..\Data\sampledata.csv ..\..\..\Data\testdata.csv

MicroAccuracy: 1
MacroAccuracy: 1
LogLoss: .1
LogLossReduction: .856


input.json
{
"Subject":"hello",
"Body":"how is it?",
"Sender":"joe@gmail.com"
}


predict .\input.json

Based on input json:
{
"Subject":"hello",
"Body":"how is it?",
"Sender":"joe@gmail.com"
}

The email is predicted to be a "friend"


Evaluating classification model
attributes to calculate model accuracy
based on a test set at the time of training
indicate how well model will perform

e.g.
CalibratedBinaryClassificationMetrics

Accuracy
Area Under ROC Curve
F1 Score
Area Under Precision-Recall Curve

MulticlassClassificationMetrics
used in the multi-class classification app

Micro Accuracy
Macro Accuracy
Log Loss
Log-Loss Reduction


Accuracy
proportion of correct to incorrect predictions in test dataset
close to 100% but not 100%
if 100% then overfitting


Area Under ROC Curve	AUC	area under curve
close to 100%
if < 50% then model needs more features / training data


F1 Score
harmonic mean of both precision and recall
close to 100% is preferred
0 = precision completely inaccurate
binary classification = 87.50%


Area Under Precision-Recall Curve	AUPRC
measure of successful prediction
value should be inspected when dataset is imbalanced
[into one classification]
close t9 100% preferred as this indicates high recall


Micro Accuracy
evaluates if every sample-class pair contributes equally to the
accuracy metric
value close to or equal 1 preferred

Macro Accuracy
evaluates if every class pair contributes equally to the
accuracy metric
value close to or equal 1 preferred

Log Loss
evaluation metric describing accuracy of the classifier
takes into account the difference btwn model prediction
and the actual classification
value close to or equal 0 preferred as indicates model prediction
on the test set is perfect

Log-Loss Reduction
evaluation metric describing accuracy of the classifier as
compared to random prediction
value close to or equal 1 preferred


SUMMARY
trained binary classification using FastTree to predict car price
and
multi-class classification using SDCA trainer to categorize emails

evalue classification model using various properties to achieve
proper evaluation of classification models


Chap.05