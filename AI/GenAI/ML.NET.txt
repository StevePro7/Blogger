ML.NET
24-Dec-2024

https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-ML.NET


Chp.01
ASIC	Application-Specific Integrated Circuits
TPU	Tensor Processing Units

2018
Machine learning to data is what refining plants are to oil.

Model Building process
- Problem statement
- Feature engineering
- Obtain dataset
- Feature extraction
- Model training
- Model evaluation


Problem statement
The five Ws of Who, What, When, Where, and Why


Feature engineering
features as
components or attributes of the problem you wish to solve
features are one of the biggest impacts on your
model's performance

feature importance
is used to determine what features are actually driving your predictions


Obtain datasets
A dataset is used to train the model on what the output
should be

overfitting
your dataset is composed of all the same data
points and your model will not be able to properly predict

A diverse but representative dataset is required for machine
learning algorithms to properly build a production-ready model


Feature extraction
Feature extraction, depending on the size of your dataset and your features,
could be one of the most time-consuming elements of the model building process


Model training
After feature extraction, you are now prepared to train your model


Model evaluation
model evaluation is to hold out a portion of your dataset for evaluation. The idea behind this
is to take known data, submit it to your trained model, and measure the efficacy of your model


Supervised ML
passing the known outputs as part of the training to the model.
labelling

Unsupervised ML
typical use case is when figuring out the input
and output labels proves to be difficult


ML Algorithms
- Binary classification
- Regression
- Anomaly detection
- Clustering
- Matrix factorization

Binary classification
output of a model trained with a binary classification algorithm will return a true or false conviction

Regression
return a real value as opposed to a binary algorithm or ones that return from a set of specific values

NB: 
linear		return predicted value
logistic	return probability of outcome occurring

Anomaly detection
looks for unexpected events in the data submitted
to the model

Clustering	Unsupervised ML
offer a unique solution to problems
where finding the closest match to related items is the desired solution.

Matrix factorization
provides a powerful and easy-to-use
algorithm for providing recommendations


ONNX
Open Neural Network eXchange

CNTK
CogNitive ToolKit


Chap.02
create + train ML model

Binary Logistic Regression Classification model
SDCA
Stochastic Dual Coordinate Ascent

SDCA = the algorithm

Project architecture
Model runs


Trainer
model building and evaluation code

Predictor
code to run predictions w/ a trained model


RestaurantFeedback
structured input to feed data pipeline
passed into training phase and trained model

label + text


RestaurantPrediction
output properties from model runs

prediction + probability + score


Trainer
load training data		LoadCSV
builds train and test set	TrainTestSplit
creates pipeline		Fit		train the model
saves model
evaluates model			Evaluate


TTS = 20% or 0.2 by default


Predictor
provides prediction support
run the model given input

load model
chapter02.mdl

create prediction object using input / output
input	RestaurantFeedback,
output	RestaurantPrediction

predict output i.e. -ve or +ve result
based on input


Evaluate model
each model type [algorithm] uses different metrics to concentrate
on when analyzing the performance of a model
e.g.
binary classification

define 4x prediction types
TN	True  negative	  properly classified Neg
TP	True  positive	  properly classified Pos
							CANCER
FN	False negative	improperly classified Neg	game over
FP	False positive	improperly classified Pos  	treatment

IMP!
remember "confusion matrix"


Metric #1	Accuracy
commonly used metric when evaluate model
calculated as ratio of correctly classified
predictions to total classifications

Metric #2	Precision
proportion of true results over all positive results in a model
e.g.
precision = 1	no FP	false positives		ideal
FP e.g. misclassifying file as malignant when is benign

Metric #3	Recall
fraction of all correct results returned by the model
e.g.
recall = 1	no FN	false negatives		ideal
FN e.g. classifying something negative when should be positive

Metric #4	F-score
utilizes both precision and recall
produces weighted average based on FP and FN

give another perspective on model performance
compared to simply looking at accuracy

range 0-1	ideal = 1


AUC
Area Under Curve
area under plotted graph with

X-axis	FP	false positives
Y-axis	TP	 true positives
e.g.
binary classifier model
returned AUC value btwn 0 and 1
i.e.	100%


Average Log Loss
expresses the penalty for wrong results in a single number
taking the difference btwn true classification and the one
the model predicts

Training Log Loss
represents the uncertainty of the model using probability
vs. the known values
NB: low numbers = better


GPT
difference btwn model and algorithm
e.g.
Binary Classification [model]

Binary Classification
type of problem where task = classify data into one of two categories

Binary Classifier
model that is trained to make these two-class predictions

Algorithm
process or method used to train the model
e.g.
logistic regression uses gradient descent to optimize model parameters
support vector machine uses optimization techniques to find best hyperplane


SUMMARY
Binary classifier	model that outputs one of two classes
Algorithm		method used to train the model

e.g.
logistic regression = 	algorithm
when logistic regression [algorithm] used to classify data into two
classes =		binary classifier



Chap.03