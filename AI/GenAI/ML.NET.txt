
24-Dec-2024

https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-ML.NET


Chp.01
ASIC	Application-Specific Integrated Circuits
TPU	Tensor Processing Units

2018
Machine learning to data is what refining plants are to oil.

Model Building process
- Problem statement
- Feature engineering
- Obtain dataset
- Feature extraction
- Model training
- Model evaluation


Problem statement
The five Ws of Who, What, When, Where, and Why


Feature engineering
features as
components or attributes of the problem you wish to solve
features are one of the biggest impacts on your
model's performance

feature importance
is used to determine what features are actually driving your predictions


Obtain datasets
A dataset is used to train the model on what the output
should be

overfitting
your dataset is composed of all the same data
points and your model will not be able to properly predict

A diverse but representative dataset is required for machine
learning algorithms to properly build a production-ready model


Feature extraction
Feature extraction, depending on the size of your dataset and your features,
could be one of the most time-consuming elements of the model building process


Model training
After feature extraction, you are now prepared to train your model


Model evaluation
model evaluation is to hold out a portion of your dataset for evaluation. The idea behind this
is to take known data, submit it to your trained model, and measure the efficacy of your model


Supervised ML
passing the known outputs as part of the training to the model.
labelling

Unsupervised ML
typical use case is when figuring out the input
and output labels proves to be difficult


ML Algorithms
- Binary classification
- Regression
- Anomaly detection
- Clustering
- Matrix factorization

Binary classification
output of a model trained with a binary classification algorithm will return a true or false conviction

Regression
return a real value as opposed to a binary algorithm or ones that return from a set of specific values

NB: 
linear		return predicted value
logistic	return probability of outcome occurring

Anomaly detection
looks for unexpected events in the data submitted
to the model

Clustering	Unsupervised ML
offer a unique solution to problems
where finding the closest match to related items is the desired solution.

Matrix factorization
provides a powerful and easy-to-use
algorithm for providing recommendations


ONNX
Open Neural Network eXchange

CNTK
CogNitive ToolKit


Chap.02
create + train ML model

Binary Logistic Regression Classification model
SDCA
Stochastic Dual Coordinate Ascent

SDCA = the algorithm

Project architecture
Model runs


Trainer
model building and evaluation code

Predictor
code to run predictions w/ a trained model


RestaurantFeedback
structured input to feed data pipeline
passed into training phase and trained model

label + text


RestaurantPrediction
output properties from model runs

prediction + probability + score


Trainer
load training data		LoadCSV
builds train and test set	TrainTestSplit
creates pipeline		Fit		train the model
saves model
evaluates model			Evaluate


TTS = 20% or 0.2 by default


Predictor
provides prediction support
run the model given input

load model
chapter02.mdl

create prediction object using input / output
input	RestaurantFeedback,
output	RestaurantPrediction

predict output i.e. -ve or +ve result
based on input


Evaluate model
each model type [algorithm] uses different metrics to concentrate
on when analyzing the performance of a model
e.g.
binary classification

define 4x prediction types
TN	True  negative	  properly classified Neg
TP	True  positive	  properly classified Pos
							CANCER
FN	False negative	improperly classified Neg	game over
FP	False positive	improperly classified Pos  	treatment

IMP!
remember "confusion matrix"


Metric #1	Accuracy
commonly used metric when evaluate model
calculated as ratio of correctly classified
predictions to total classifications

Metric #2	Precision
proportion of true results over all positive results in a model
e.g.
precision = 1	no FP	false positives		ideal
FP e.g. misclassifying file as malignant when is benign

Metric #3	Recall
fraction of all correct results returned by the model
e.g.
recall = 1	no FN	false negatives		ideal
FN e.g. classifying something negative when should be positive

Metric #4	F-score
utilizes both precision and recall
produces weighted average based on FP and FN

give another perspective on model performance
compared to simply looking at accuracy

range 0-1	ideal = 1


AUC
Area Under Curve
area under plotted graph with

X-axis	FP	false positives
Y-axis	TP	 true positives
e.g.
binary classifier model
returned AUC value btwn 0 and 1
i.e.	100%


Average Log Loss
expresses the penalty for wrong results in a single number
taking the difference btwn true classification and the one
the model predicts

Training Log Loss
represents the uncertainty of the model using probability
vs. the known values
NB: low numbers = better


GPT
difference btwn model and algorithm
e.g.
Binary Classification [model]

Binary Classification
type of problem where task = classify data into one of two categories

Binary Classifier
model that is trained to make these two-class predictions

Algorithm
process or method used to train the model
e.g.
logistic regression uses gradient descent to optimize model parameters
support vector machine uses optimization techniques to find best hyperplane


SUMMARY
Binary classifier	model that outputs one of two classes
Algorithm		method used to train the model

e.g.
logistic regression = 	algorithm
when logistic regression [algorithm] used to classify data into two
classes =		binary classifier



Chap.03
Trainers

Regression Model application
Linear Regression	predict employee attrition
Logistic Regression	perform basic static file analysis

regression models
9x linear regression trainers

FastTreeRegressionTrainer
FastTreeTweedieTrainer
FastForestRegressionTrainer
GamRegressionTrainer
LbfgsPoissonRegressionTrainer
LightGbmRegressionTrainer
OlsTrainer
OnlineGradientDescentTrainer
SdcaRegressionTrainer


binary logistic regression trainers:
4x logistic regression trainers

LbfgsLogisticRegressionBinaryTrainer
SdcaLogisticRegressionBinaryTrainer
SdcaNonCalibratedBinaryTrainer
SymbolicSgdLogisticRegressionBinaryTrainer

logistic regression
boolean value
pre-defined range of values

linear regression
unknown numeric value e.g. employment duration


Choose linear regression trainer
FastTree	utilize neighbor joining
LightGBM	utilize GOSS Gradient-based One Side Sampling


Choose logistic regression trainer
all return binary classification

L-BFGS		train + predict in low memory environment
SDCA		optimized for scalability in training
Symbolic	minimizes the error function


Regression Model application #1
Linear Regression

var trainer = MlContext.Regression.Trainers.Sdca();

SDCA
Stochastic	unpredictability to predict error function
Dual Co-ord	two variables coupled when training the model
Ascent		maximizing the value of the error function


Trainer
NormalizeMeanVariance
normalize on both mean and variance
subtract mean from input data and divide value by the variance
nullify outliers so model isn't skewed

Program
train
Project Properties | Debug
train ..\..\..\Data\sampledata.csv

Loss Function: 266.27
Mean Absolute Error: 14.01
Mean Squared Error: 266.27
RSquared: -0.51
Root Mean Squared Error: 16.32

predict
Project Properties | Debug
predict ..\..\..\Data\input.json

Based on input json:
{
"durationInMonths": 0.0,
"isMarried": 0,
"bsDegree": 1,
"msDegree": 1,
"yearsExperience": 2,
"ageAtHire": 29,
"hasKids": 0,
"withinMonthOfVesting": 0,
"deskDecorations": 1,
"longCommute": 1
}
The employee is predicted to work 25.73 months


Regression Model application #2
Logistic regression

var trainer = MlContext.BinaryClassification.Trainers.SdcaLogisticRegression();

true being malicious and false being benign


Trainer
FeaturizeText transform builds NGrams from strings data extracted from files
NGrams
popular method to create vectors from a string and feed into the model
breaks longer string into ranges of characters based on NGram parameter

extract temp_data
train ..\..\..\Data\sampledata.csv

Evaluating regression model
poorly trained model with only provide inaccurate predictions

calcluate model accuracy based on test set at time of training
gives an idea how well model will perform in Prod environment


Regression Metrics
Loss function
Mean absolute error
Mean squared error
R-squared
Root mean squared error


Loss function
set when regression trainer initialized
SDCA default to SquaredLoss NOT TweedieLoss or PoissonLoss


Mean squared error	MSE
measure of average of the square of errors
best used to evaluate models when outliers are critical to prediction


Mean absolute error	MAE
sums the distances btwn points and the prediction lines as
opposed to computing the mean

IMPORTANT
mean bias error
two data points equidistant from the line
one above and one below - this would balance out with
positive and negative value


R-squared		coefficient of determination
represent how accurate the prediction is compared to the test set
calculated by taking the sumo f the distance btwn every data point
and the mean squared - subtracting them and then squaring it

IMP
high values != model performance	overfitting
e.g.
lot of features fed to the model = model more complex
OR
simply not enough diversity in the training and test sets
thus test set = same range of values = overfitting


Root mean squared error	RMSE
distance btwn predicted and actual values
RMSE takes a mean of all those distances, square that value,
and takes the square root
value < 180 = good model

Summary
linear regression
application using SDCA and ML.NET to predict employee attrition

logistic regression
application using SDCA and ML.NET to provide file classification



Chap.04
binary classification
01. car price good or not				0 or 1
02. multi-class classification app categorizes email	range labels

binary classification models

AveragedPerceptronTrainer
SdcaLogisticRegressionBinaryTrainer
SdcaNonCalibratedBinaryTrainer	
SymbolicSgdLogisticRegressionBinaryTrainer
LbfgsLogisticRegressionBinaryTrainer
LightGbmBinaryTrainer
FastTreeBinaryTrainer
FastForestBinaryTrainer
GamBinaryTrainer
FieldAwareFactorizationMachineTrainer
PriorTrainer
LinearSvmTrainer


mulit-class classifiers

LightGbmMulticlassTrainer
SdcaMaximumEntropyMulticlassTrainer
SdcaNonCalibratedMulticlassTrainer
LbfgsMaximumEntropyMulticlassTrainer
NaiveBayesMulticlassTrainer
OneVersusAllTrainer
PairwiseCouplingTrainer

Ex
01.	FastTreeBinaryTrainer
02.	SdcaMaximumEntropyMulticlassTrainer

remember prediction output type will help decide btwn
binary or multi-class classification

best
binary classification trainers
SDCA, LightGBM, and FastTree

multi-class classification trainers
LightGBM and SDCA


Ex01
FastTree based on MART
Multiple Additive Regression Trees
gradient boosting algorithm

Gradient boosting
popular technique in which a series of trees are built in a
step-wise manner before ultimately selecting the best tree

MART
learning an ensemble or regression trees that use scaler
values in their leaves

NB: output
PredictedLabel property contains our classification result


Trainer uses NormalizeMeanVariance

BV:
tweak no. leaves and no. trees to see how both the model
metrics and your prediction probability percentage changes


Program
train ..\..\..\Data\sampledata.csv ..\..\..\Data\testdata.csv

Accuracy: 88.89%
Area Under Curve: 100.00%
Area under Precision recall Curve: 100.00%
F1Score: 87.50%
LogLoss: 2.19
LogLossReduction: -1.19
PositivePrecision: 1
PositiveRecall: .78
NegativePrecision: .82
NegativeRecall: 100.00%

input.json
predict input.json

Based on input json:
{
"HasSunroof":0,
"HasAC":0,
"HasAutomaticTransmission":0,
"Amount":1300
}
The car price is a good deal, with a 100% confidence


Ex02	emails
orders
spam
friend

SdcaMaximumEntropy trainer
SDCA
uses empirical risk minimization which optimizes based
on the training data
potential outliers or anomalies to affect the predict performance
thus
provide trainer w/ ample sampling data to avoid overfitting and
potential errors when predicting the ddata
NB:
SdcaMaximumEntropy trainer
does require normalization

pipeline mapping input properties to FeaturizeText transformations
before appending the SDCA trainer

Program
train ..\..\..\Data\sampledata.csv ..\..\..\Data\testdata.csv

MicroAccuracy: 1
MacroAccuracy: 1
LogLoss: .1
LogLossReduction: .856


input.json
{
"Subject":"hello",
"Body":"how is it?",
"Sender":"joe@gmail.com"
}


predict .\input.json

Based on input json:
{
"Subject":"hello",
"Body":"how is it?",
"Sender":"joe@gmail.com"
}

The email is predicted to be a "friend"


Evaluating classification model
attributes to calculate model accuracy
based on a test set at the time of training
indicate how well model will perform

e.g.
CalibratedBinaryClassificationMetrics

Accuracy
Area Under ROC Curve
F1 Score
Area Under Precision-Recall Curve

MulticlassClassificationMetrics
used in the multi-class classification app

Micro Accuracy
Macro Accuracy
Log Loss
Log-Loss Reduction


Accuracy
proportion of correct to incorrect predictions in test dataset
close to 100% but not 100%
if 100% then overfitting


Area Under ROC Curve	AUC	area under curve
close to 100%
if < 50% then model needs more features / training data


F1 Score
harmonic mean of both precision and recall
close to 100% is preferred
0 = precision completely inaccurate
binary classification = 87.50%


Area Under Precision-Recall Curve	AUPRC
measure of successful prediction
value should be inspected when dataset is imbalanced
[into one classification]
close t9 100% preferred as this indicates high recall


Micro Accuracy
evaluates if every sample-class pair contributes equally to the
accuracy metric
value close to or equal 1 preferred

Macro Accuracy
evaluates if every class pair contributes equally to the
accuracy metric
value close to or equal 1 preferred

Log Loss
evaluation metric describing accuracy of the classifier
takes into account the difference btwn model prediction
and the actual classification
value close to or equal 0 preferred as indicates model prediction
on the test set is perfect

Log-Loss Reduction
evaluation metric describing accuracy of the classifier as
compared to random prediction
value close to or equal 1 preferred


SUMMARY
trained binary classification using FastTree to predict car price
and
multi-class classification using SDCA trainer to categorize emails

evalue classification model using various properties to achieve
proper evaluation of classification models


Chap.05
k-means
unsupervised ML algorithm
e.g.
type of file via content

use cases
output categorizes similar outputs into groups of similar data points

k-means trainer
based on Yinyang method not classic k-means impl
all input normalized into single feature vector

Clustering
distances btwn data point and each cluster are measures which cluster
the model will return
centroid = center point of each of htese clusters
k-means
calculates the distance to the data point
the smallest of these values is the predicted cluster


PredictedClusterId property stores the closest cluster found
prediction returns, including the Euclidean distances

Program
extract ..\..\..\TrainingData\ ..\..\..\TestData\

train ..\..\..\Data\sampledata.csv ..\..\..\Data\testdata.csv
Average Distance: NaN
Davies Bould Index: 0
Normalized Mutual Information: NaN

predict .\chapter05.exe

Based on input file: .\chapter05.exe

Feature Extraction: 0,1,1,0

The file is predicted to be a Executable

Distances from all clusters:
Executable: 0
Script: 2
Document: 2


Evaluate k-means model
attributes calculate model accuracy based on test set at the time
of training to give you an idea how well model will perform in Prod

e.g.
ClusteringMetrics

Average distance
The Davies-Bouldin index
Normalized mutual information


Average distance		Avg score
distance from center of cluster to test data
if value = 0 then features create distinct clusters
if poor prediction performance then increase the no. clusters

The Davies-Bouldin index
measure for the quality of the clustering
ranging from 0 to 1
if value = 0 then ideal

Normalized mutual information
used to measure mututal dependence of the feature variables
value range 0 to 1
if closer to or equal to 1 = ideal

Summary
clustering app useing k-means to predict file type
evaluate k-means clustering model to achieve proper evaluation
of a k-means clustering model


Chap.06
Ex
login attempts
time series in network traffic

Anomaly Detection
Unsupervised ML algorithm
train on data and look for data that does not fit the normal data

AD
data is available but it is unknown whether there is an anomaly in the data

examples
sales forecasting
stock market
fraud detection
cyber security: network traffic login histroy

randomized PCA trainer
requires normalization of the values

PredictedLabel
indicates a valid anomalhy based on the threshold set

NB:
AD algorithm uses eigenvectors to estimate the subspace containing
the normal class and then computes the normalized difference btwn
the actual feature vector and the projected feature in that subspace

Algorithm finds edge cases if computed error is not close to 0


Time series
transforms are grouped in the TimeSeriesCatalog class

DetectAnomalyBySrCnn
DetectChangePointBySsa
DetectIidChangePoint
DetectIidSpike
DetectSpikeBySsa
ForecastBySsa

SSA
Singular Spectrum Analysis

Time series example looks for spikes in network transfer over time via
DetectSpikeBySsa

Ex06 time series
train ..\..\..\Data\sampledata.csv

predict ..\..\..\Data\testdata.csv
Based on input file (..\..\..\Data\testdata.csv):
HOST: laptop TIMESTAMP: 22/11/2019 11:13:23 TRANSFER: 1000 ALERT: 0 SCORE: 46.07 P-VALUE: 0.50
HOST: laptop TIMESTAMP: 23/11/2019 11:13:23 TRANSFER: 1100 ALERT: 0 SCORE: 131.36 P-VALUE: 0.00
HOST: laptop TIMESTAMP: 24/11/2019 11:13:23 TRANSFER: 1200 ALERT: 0 SCORE: 180.44 P-VALUE: 0.06
HOST: laptop TIMESTAMP: 25/11/2019 11:13:23 TRANSFER: 1300 ALERT: 0 SCORE: 195.42 P-VALUE: 0.17
HOST: laptop TIMESTAMP: 26/11/2019 11:13:23 TRANSFER: 1400 ALERT: 0 SCORE: 201.15 P-VALUE: 0.22
HOST: laptop TIMESTAMP: 27/11/2019 11:13:23 TRANSFER: 3000 ALERT: 1 SCORE: 1365.42 P-VALUE: 0.00
HOST: laptop TIMESTAMP: 28/11/2019 11:13:23 TRANSFER: 1500 ALERT: 0 SCORE: -324.58 P-VALUE: 0.11
HOST: laptop TIMESTAMP: 29/11/2019 11:13:23 TRANSFER: 1600 ALERT: 0 SCORE: -312.93 P-VALUE: 0.25


alert	nonzero = anomaly
score	numeric representation of the anomaly score
p-value	distance btwn current point and the average point	0 = spike


Ex	AD app
login anomaly detector

AD classification using the randomized PCA trainer
RandomizedPcaTrainer

evaluate the trained model using the test dataset

Program
train ..\..\..\Data\sampledata.csv ..\..\..\Data\testdata.csv

Area Under Curve: 78.12%
Detection at FP Count: 1

input.json
{
  "UserID": 0, 
  "CorporateNetwork": 1, 
  "HomeNetwork": 0,
  "WithinWorkHours": 1, 
  "WorkDay": 1
}

predict input.json

Based on input json:
{
  "UserID": 0,
  "CorporateNetwork": 1,
  "HomeNetwork": 0,
  "WithinWorkHours": 1,
  "WorkDay": 1
}
The login history is normal, with a 0.00 outlier score

Evaluating randomized PCA model
poorly trained model will only provide inaccurate predictions

popular attributes to calculate model accuracy based on a
test set at the time of training to give idea how well the
model will perform in PROD

AnomalyDetectionMetrics
Area under the ROC curve
Detection rate at false positive count

Area under the ROC curve		similar to regression
ROC	Receiver Operating Characteristic
computed area = chance algorithm [randomized PCA] scores positive
instance higher that negative one both chosen randomly to better
evaluate the data

Number close to 100% is ideal value
value close to 0%  shows false positives

IMPORTANT
Pg.129
shows area under data curve in btwn the random guessing line
is the area under the ROC curve data metric


Detection rate at false positive count
detection rate of K false positives

FP false positive in an anomaly detection would be to consider a
data point an anomaly when in fact it was not

rate computed by
detection rate of K falose positives = X / Y

X	top test samples based on the scores in anomaly detection
	top true positives	more likely to be anomalies

Y	total number of anomalies in the test data regardless of score value
	not filtering points that look suspicious or not
	could be high if no. FP high in training data

In Prod ensure data close to Prod as poss to avoid overfitting or
underfitting to anomalies

SUMMARY
anomaly detection using randomized PCA algorithm
evaluate AD model using properties exposed to achieve proper
evaluation of AD model


Chap.07
matrix factorization
unsupervised ML algorithm

predict music recommendations based on sample training data

applications
Music recommendations
Product recommendations
Movie recommendations
Book recommendations

cold start problem

matrix factorization trainer requires both normalization of the values and
caching

Program
train ..\..\..\Data\sampledata.csv ..\..\..\Data\testdata.csv

Matrix Factorization Evaluation:

Loss Function: 0.129
Mean Absolute Error: 0.279
Mean Squared Error: 0.129
R Squared: 0.929
Root Mean Squared Error: 0.359

input.json
predict input.json

Based on input:
Label: 3 | MusicID: 4 | UserID: 10
The music is not recommended


Evaluate matrix factorization model

RegressionMetrics
Loss function
Mean Squared Error (MSE)
Mean Absolute Error (MAE)
R-squared
Root Mean Squared Error (RMSE)


Loss function
default SquaredLossRegression


Mean Squared Error (MSE)
measure of the average of the squares of the errors
smaller the value the better the fitting = more accuated predictions
best used to evaluate models when outliers critical to prediction output


Mean Absolute Error (MAE)
sums the differences btwn the points and the prediction lines
as opposed to computing the mean

best used to evaluate models when outliers are considered anomalies
and shouldn't be counted in evaluating model's performance

NB: MBE	Mean Bias Error
when equal +ve distance and -ve distance = balanced out


R-squared
co-efficient of determination
how well prediction compares to the test set

R- squared = difference btwn each predicted value and actual value, 
squaring that difference, then summing squares for each pair of points


Root Mean Squared Error (RMSE)
red dots	actual values from test set
blue dots	predicted values

X = distance btwn predicted and actual values

RMSE takes mean of all those distance, squares that value
and takes the square root

value < 180 = good model

SUMMARY
matrix factorization application to predict music recommendations
evaluate matrix factorization model via various properties exposed
to achieve a proper evaluation of a matrix factorization model


Chap.08
action training TrainingFileName ..\..\..\Data\sampledata.csv TestingFileName ..\...\..\Data\testdata.csv

action predict
Given a stock price of $101, the next 5 values are predicted to be: $128, $925, $140, $145, $1057
Given a stock price of $102, the next 5 values are predicted to be: $924, $138, $136, $1057, $158
Given a stock price of $300, the next 5 values are predicted to be: $136, $134, $852, $156, $150
Given a stock price of $40, the next 5 values are predicted to be: $133, $795, $122, $149, $864
Given a stock price of $30, the next 5 values are predicted to be: $767, $111, $114, $837, $122
Given a stock price of $400, the next 5 values are predicted to be: $105, $102, $676, $116, $108
Given a stock price of $55, the next 5 values are predicted to be: $97, $594, $91, $103, $645
Given a stock price of $69, the next 5 values are predicted to be: $557, $81, $87, $605, $90
Given a stock price of $430, the next 5 values are predicted to be: $76, $78, $515, $84, $85


Chap.09
trainingfolderpath ..\..\..\..\TrainingData\ testingfolderpath ..\..\..\..\TestData\

action training trainingfilename ..\..\..\..\sampledata.data testingfilename ..\..\..\..\testdata.data
