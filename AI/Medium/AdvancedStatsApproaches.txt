20 Advanced Statistical Approaches Every Data Scientist Should Know
13-Feb-2025

https://medium.com/@sarowar.saurav10/20-advanced-statistical-approaches-every-data-scientist-should-know-ccc70ae4df28

1. Bayesian Inference
2. Maximum Likelihood Estimation (MLE)
3. Hypothesis Testing (t-test)
4. Analysis of Variance (ANOVA)
5. Principal Component Analysis (PCA)
6. Factor Analysis
7. Cluster Analysis (K-means)
8. Bootstrapping
9. Time Series Analysis (ARIMA)
10. Survival Analysis
11. Multivariate Regression (Multiple Linear Regression)
12. Ridge/Lasso Regression
13. Logistic Regression
14. Mixed Effects Models
15. Nonparametric Tests (Mann-Whitney U)
16. Monte Carlo Simulation
17. Markov Chain Monte Carlo (MCMC)
18. Robust Regression
19. Copulas
20. Generalized Additive Models (GAMs)




5. Principal Component Analysis (PCA)
PCA reduces the dimensionality of data by projecting it onto new orthongonal axes (principal components) that capture the most variance

import numpy as np
from sklearn.decomposition import PCA

# Synthetic data: 100 samples with 10 features
X = np.random.rand(100, 10)
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)
print("Explained variance ratios:", pca.explained_variance_ratio_)
print("Reduced shape:", X_reduced.shape)



7. Cluster Analysis (K-means)
Clustering partitions data into homogeneous groups (clusters) based on similarity
K-means is a popular centroid-based clustering technique

import numpy as np
from sklearn.cluster import KMeans

# Synthetic data: 200 samples, 2D
X = np.random.rand(200, 2)
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)
print("Cluster centers:", kmeans.cluster_centers_)
print("Cluster labels:", kmeans.labels_[:10])



8. Bootstrapping
Bootstrapping involves repeatedly sampling with replacement from your dataset to estimate the distribution (and uncertainty) of a statistic (e.g. mean, median)

import numpy as np

np.random.seed(42)
data = np.random.normal(50, 5, size=100)  # Original sample
def bootstrap_mean_ci(data, n_bootstraps=1000, ci=95):
    means = []
    n = len(data)
    for _ in range(n_bootstraps):
        sample = np.random.choice(data, size=n, replace=True)
        means.append(np.mean(sample))
    lower = np.percentile(means, (100-ci)/2)
    upper = np.percentile(means, 100 - (100-ci)/2)
    return np.mean(means), (lower, upper)
mean_estimate, (lower_ci, upper_ci) = bootstrap_mean_ci(data)
print(f"Bootstrap Mean: {mean_estimate:.2f}")
print(f"{95}% CI: [{lower_ci:.2f}, {upper_ci:.2f}]")



11. Multivariate Regression (Multiple Linear Regression)
Multiple linear regression models the relationship between a dependent variable and multiple independent variables

import numpy as np
from sklearn.linear_model import LinearRegression

# Synthetic data: price = 100 + 2*rooms + 0.5*sqft + noise
np.random.seed(42)
rooms = np.random.randint(1, 5, 100)
sqft = np.random.randint(500, 2500, 100)
price = 100 + 2*rooms + 0.5*sqft + np.random.normal(0, 50, 100)
X = np.column_stack([rooms, sqft])
y = price
model = LinearRegression()
model.fit(X, y)
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)


12. Ridge/Lasso Regression
Ridge
adds an L2 penalty to reduce overfitting by shrinking coefficients

Lasso
adds an L1 penalty which can drive some coefficients to zero effectively performing feature selection

import numpy as np
from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import train_test_split

# Synthetic data
np.random.seed(42)
X = np.random.rand(100, 10)
y = X[:, 0]*5 + X[:, 1]*3 + np.random.normal(0, 0.1, 100)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
ridge = Ridge(alpha=1.0).fit(X_train, y_train)
lasso = Lasso(alpha=0.1).fit(X_train, y_train)
print("Ridge coefficients:", ridge.coef_)
print("Lasso coefficients:", lasso.coef_)



13. Logistic Regression
Logistic Regression is used for binary classification modeling the probability of a certain class or event existing

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

np.random.seed(42)
X = np.random.rand(100, 5)
y = np.random.randint(0, 2, 100)  # Binary labels
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
model = LogisticRegression()
model.fit(X_train, y_train)
accuracy = model.score(X_test, y_test)
print("Accuracy:", accuracy)


16. Monte Carlo Simulation
Monte Carlo simulations use repeated random sampling to estimate the probability of different outcomes under uncertainty

import numpy as np

np.random.seed(42)
n_samples = 10_000_00
xs = np.random.rand(n_samples)
ys = np.random.rand(n_samples)
# Points within the unit circle
inside_circle = (xs**2 + ys**2) <= 1.0
pi_estimate = inside_circle.sum() * 4 / n_samples
print("Estimated Ï€:", pi_estimate)


20. Generalized Additive Models (GAMs)
GAMs extend linear models by allowing for non-linear functions of predictors while maintaining additivity.  They're more flexible than linear regression but still interpretable

!pip install pygam

import numpy as np
from pygam import LinearGAM, s

# Synthetic data
np.random.seed(42)
X = np.random.rand(200, 1) * 10
y = 2 + 3*np.sin(X).ravel() + np.random.normal(0, 0.5, 200)
gam = LinearGAM(s(0)).fit(X, y)
XX = np.linspace(0, 10, 100)
preds = gam.predict(XX)
print("Coefficients:", gam.summary())